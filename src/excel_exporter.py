"""
Excel Exporter for OceanBase capacity assessment reports
Exports multi-tab Excel workbooks with comprehensive reports
"""
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter


class ExcelExporter:
    """Export OceanBase data to Excel format with multiple tabs"""

    def __init__(self, output_dir: str = 'output'):
        """
        Initialize Excel Exporter

        Args:
            output_dir: Base directory for output files
        """
        self.base_output_dir = Path(output_dir)
        self.base_output_dir.mkdir(exist_ok=True, parents=True)

    def create_dated_directory(self, report_frequency: str = 'Daily') -> Path:
        """
        Create a dated directory structure: output/{YYYYMMDD}/{Daily|Weekly|Monthly}

        Args:
            report_frequency: 'Daily', 'Weekly', or 'Monthly'

        Returns:
            Path to the created directory
        """
        date_str = datetime.now().strftime('%Y%m%d')
        dated_dir = self.base_output_dir / date_str / report_frequency
        dated_dir.mkdir(exist_ok=True, parents=True)
        return dated_dir

    def _apply_header_formatting(self, worksheet, num_columns: int):
        """
        Apply professional formatting to worksheet headers

        Args:
            worksheet: openpyxl worksheet object
            num_columns: Number of columns in the sheet
        """
        # Header styling
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF", size=11)
        header_alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)

        # Border styling
        thin_border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )

        # Apply to header row
        for col_num in range(1, num_columns + 1):
            cell = worksheet.cell(row=1, column=col_num)
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = header_alignment
            cell.border = thin_border

        # Auto-adjust column widths
        for col_num in range(1, num_columns + 1):
            column_letter = get_column_letter(col_num)
            max_length = 0
            column_cells = worksheet[column_letter]

            for cell in column_cells[:20]:  # Check first 20 rows for width
                try:
                    if cell.value:
                        max_length = max(max_length, len(str(cell.value)))
                except:
                    pass

            adjusted_width = min(max_length + 2, 50)  # Cap at 50 characters
            worksheet.column_dimensions[column_letter].width = adjusted_width

        # Freeze the header row
        worksheet.freeze_panes = 'A2'

    def export_consolidated_report(
        self,
        capacity_csv_path: str,
        tenants_csv_path: str,
        report_frequency: str = 'Daily',
        custom_filename: Optional[str] = None
    ) -> str:
        """
        Export consolidated report with multiple tabs in a single Excel file
        Reads from CSV files generated by the daily/weekly/monthly extraction

        Args:
            capacity_csv_path: Path to capacity assessment CSV file
            tenants_csv_path: Path to tenants CSV file
            report_frequency: 'Daily', 'Weekly', or 'Monthly'
            custom_filename: Optional custom filename (without extension)

        Returns:
            Path to the created Excel file
        """
        # Read CSV files
        try:
            df_capacity = pd.read_csv(capacity_csv_path)
            capacity_data = df_capacity.to_dict('records')
        except Exception as e:
            print(f"Warning: Could not read capacity CSV {capacity_csv_path}: {e}")
            df_capacity = pd.DataFrame()
            capacity_data = []

        try:
            df_tenants = pd.read_csv(tenants_csv_path)
            tenants_data = df_tenants.to_dict('records')
        except Exception as e:
            print(f"Warning: Could not read tenants CSV {tenants_csv_path}: {e}")
            df_tenants = pd.DataFrame()
            tenants_data = []

        # Create dated directory
        output_dir = self.create_dated_directory(report_frequency)

        # Generate filename
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        if custom_filename:
            filename = f"{custom_filename}.xlsx"
        else:
            filename = f"OceanBase_{report_frequency}_Report_{timestamp}.xlsx"

        filepath = output_dir / filename

        # Create Excel writer
        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
            # Tab 1: Capacity Assessment
            if not df_capacity.empty:
                df_capacity_ordered = self._reorder_capacity_columns(df_capacity)
                df_capacity_ordered.to_excel(writer, sheet_name='Capacity Assessment', index=False)

                # Apply formatting
                worksheet = writer.sheets['Capacity Assessment']
                self._apply_header_formatting(worksheet, len(df_capacity_ordered.columns))

            # Tab 2: Tenants Report
            if not df_tenants.empty:
                df_tenants_ordered = self._reorder_tenants_columns(df_tenants)
                df_tenants_ordered.to_excel(writer, sheet_name='Tenants Report', index=False)

                # Apply formatting
                worksheet = writer.sheets['Tenants Report']
                self._apply_header_formatting(worksheet, len(df_tenants_ordered.columns))

            # Tab 3: Summary Statistics
            if capacity_data:
                summary_df = self._generate_summary_statistics(capacity_data)
                summary_df.to_excel(writer, sheet_name='Summary Statistics', index=False)

                # Apply formatting
                worksheet = writer.sheets['Summary Statistics']
                self._apply_header_formatting(worksheet, len(summary_df.columns))

        print(f"âœ“ Consolidated {report_frequency} report saved to: {filepath}")
        print(f"  - Capacity Assessment: {len(capacity_data)} instances")
        print(f"  - Tenants Report: {len(tenants_data)} tenants")
        print(f"  - Report Type: {report_frequency}")

        return str(filepath)

    def _reorder_capacity_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Reorder capacity assessment columns for better readability and rename to clarify units"""
        # Rename columns to clarify that CPU/Memory metrics are percentages
        rename_map = {
            'cpu_avg': 'cpu_utilization_avg_%',
            'cpu_min': 'cpu_utilization_min_%',
            'cpu_max': 'cpu_utilization_max_%',
            'cpu_p95': 'cpu_utilization_p95_%',
            'memory_avg': 'memory_utilization_avg_%',
            'memory_min': 'memory_utilization_min_%',
            'memory_max': 'memory_utilization_max_%',
            'memory_p95': 'memory_utilization_p95_%',
            'disk_avg': 'disk_utilization_avg_%',
            'disk_min': 'disk_utilization_min_%',
            'disk_max': 'disk_utilization_max_%',
            'disk_p95': 'disk_utilization_p95_%'
        }

        df_renamed = df.rename(columns=rename_map)

        column_order = [
            'instance_id', 'instance_name', 'status', 'series',

            # CPU Capacity Allocation (Capacity Center fields)
            'total_cpu', 'allocated_cpu', 'available_cpu',

            # Memory Capacity Allocation (Capacity Center fields)
            'total_memory', 'allocated_memory', 'available_memory',

            # Storage (Data Disk) Capacity Allocation (Capacity Center fields)
            'total_storage', 'allocated_storage', 'actual_data_usage', 'available_storage',

            # Log Disk Capacity Allocation (Capacity Center fields)
            'total_log_disk', 'allocated_log_disk', 'available_log_disk',

            # Instance metadata
            'disk_type',

            # Disk utilization (from CloudMonitor)
            'disk_utilization_pct',

            # CPU utilization metrics (percentages 0-100%, from CloudMonitor)
            'cpu_utilization_avg_%', 'cpu_utilization_min_%', 'cpu_utilization_max_%', 'cpu_utilization_p95_%',

            # Memory utilization metrics (percentages 0-100%, from CloudMonitor)
            'memory_utilization_avg_%', 'memory_utilization_min_%', 'memory_utilization_max_%', 'memory_utilization_p95_%',

            # Disk utilization metrics (percentages 0-100%, from CloudMonitor)
            'disk_utilization_avg_%', 'disk_utilization_min_%', 'disk_utilization_max_%', 'disk_utilization_p95_%',

            # Instance info
            'create_time'
        ]

        # Columns to exclude from Capacity Assessment tab
        excluded_columns = [
            # Memstore metrics (instance-level, not needed)
            'memstore_percent_avg', 'memstore_percent_min', 'memstore_percent_max', 'memstore_percent_p95',
            # QPS/TPS metrics (aggregate, not detailed enough)
            'qps_avg', 'qps_min', 'qps_max', 'qps_p95',
            'tps_avg', 'tps_min', 'tps_max', 'tps_p95',
            'qps_rt_ms_avg', 'qps_rt_ms_min', 'qps_rt_ms_max', 'qps_rt_ms_p95',
            'tps_rt_ms_avg', 'tps_rt_ms_min', 'tps_rt_ms_max', 'tps_rt_ms_p95',
            # I/O bytes metrics (bytes per sec, too granular)
            'io_read_bytes_per_sec_avg', 'io_read_bytes_per_sec_min', 'io_read_bytes_per_sec_max', 'io_read_bytes_per_sec_p95',
            'io_write_bytes_per_sec_avg', 'io_write_bytes_per_sec_min', 'io_write_bytes_per_sec_max', 'io_write_bytes_per_sec_p95'
        ]

        # Only include columns that exist
        available_columns = [col for col in column_order if col in df_renamed.columns]
        remaining_columns = [col for col in df_renamed.columns
                           if col not in column_order and col not in excluded_columns]
        final_columns = available_columns + remaining_columns

        return df_renamed[final_columns]

    def _reorder_tenants_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Reorder tenant columns for better readability"""
        column_order = [
            'instance_id', 'instance_name', 'tenant_id', 'tenant_name',
            'tenant_mode',

            # Tenant Resource Allocation (Capacity Center fields - from DescribeTenant API)
            'tenant_allocated_cpu',
            'tenant_allocated_memory',
            'tenant_actual_disk_usage',

            # CPU Usage Metrics (from CloudMonitor API)
            'cpu_usage_percent_avg', 'cpu_usage_percent_max', 'cpu_usage_percent_min', 'cpu_usage_percent_p95',

            # Log Disk Allocation
            'tenant_allocated_log_disk',

            # Tenant Connection Information
            'max_connections',
            'active_sessions_avg',
            'connection_utilization_pct',

            # Memory metrics (from CloudMonitor API) - percentage only
            'memory_usage_percent_avg', 'memory_usage_percent_max', 'memory_usage_percent_min',

            # Session/Connection metrics (from CloudMonitor API)
            'active_sessions_max', 'active_sessions_min',

            # Tenant info
            'create_time'
        ]

        # Only include columns that exist
        available_columns = [col for col in column_order if col in df.columns]
        remaining_columns = [col for col in df.columns if col not in column_order]
        final_columns = available_columns + remaining_columns

        return df[final_columns]

    def _generate_summary_statistics(self, capacity_data: List[Dict]) -> pd.DataFrame:
        """Generate summary statistics from capacity data"""
        df = pd.DataFrame(capacity_data)

        summary_rows = []

        # Total instances count
        summary_rows.append({
            'Metric': 'Total Instances',
            'Value': len(df),
            'Unit': 'count'
        })

        # Online instances
        if 'status' in df.columns:
            online_count = len(df[df['status'] == 'ONLINE'])
            summary_rows.append({
                'Metric': 'Online Instances',
                'Value': online_count,
                'Unit': 'count'
            })

        # Total CPU
        if 'total_cpu' in df.columns:
            total_cpu = df['total_cpu'].sum()
            summary_rows.append({
                'Metric': 'Total CPU Capacity',
                'Value': round(total_cpu, 2),
                'Unit': 'cores'
            })

        # Total Memory
        if 'total_memory' in df.columns:
            total_memory = df['total_memory'].sum()
            summary_rows.append({
                'Metric': 'Total Memory Capacity',
                'Value': round(total_memory, 2),
                'Unit': 'GB'
            })

        # Total Storage
        if 'total_storage' in df.columns:
            total_storage = df['total_storage'].sum()
            summary_rows.append({
                'Metric': 'Total Storage Capacity',
                'Value': round(total_storage, 2),
                'Unit': 'GB'
            })

        # Used Storage
        if 'used_storage' in df.columns:
            used_storage = df['used_storage'].sum()
            summary_rows.append({
                'Metric': 'Total Used Storage',
                'Value': round(used_storage, 2),
                'Unit': 'GB'
            })

        # Average Disk Utilization
        if 'disk_utilization_pct' in df.columns:
            avg_disk_util = df['disk_utilization_pct'].mean()
            summary_rows.append({
                'Metric': 'Average Disk Utilization',
                'Value': round(avg_disk_util, 2),
                'Unit': '%'
            })

        # Average CPU Utilization
        if 'cpu_avg' in df.columns:
            avg_cpu = df['cpu_avg'].mean()
            summary_rows.append({
                'Metric': 'Average CPU Utilization',
                'Value': round(avg_cpu, 2),
                'Unit': '%'
            })

        # Average Memory Utilization
        if 'memory_avg' in df.columns:
            avg_memory = df['memory_avg'].mean()
            summary_rows.append({
                'Metric': 'Average Memory Utilization',
                'Value': round(avg_memory, 2),
                'Unit': '%'
            })

        return pd.DataFrame(summary_rows)

    def export_weekly_report(
        self,
        daily_reports: List[Dict],
        week_start_date: Optional[datetime] = None
    ) -> str:
        """
        Export weekly aggregated report

        Args:
            daily_reports: List of daily report data dictionaries
            week_start_date: Start date of the week (defaults to current week)

        Returns:
            Path to the created Excel file
        """
        if not week_start_date:
            week_start_date = datetime.now() - timedelta(days=datetime.now().weekday())

        week_str = week_start_date.strftime('%Y_W%U')
        filename = f"OceanBase_Weekly_Report_{week_str}"

        # Aggregate weekly data (you can customize aggregation logic)
        weekly_capacity = self._aggregate_reports(daily_reports, 'weekly')

        return self.export_consolidated_report(
            capacity_data=weekly_capacity,
            tenants_data=[],  # You can aggregate tenant data similarly
            report_frequency='Weekly',
            custom_filename=filename
        )

    def export_monthly_report(
        self,
        daily_reports: List[Dict],
        month_start_date: Optional[datetime] = None
    ) -> str:
        """
        Export monthly aggregated report

        Args:
            daily_reports: List of daily report data dictionaries
            month_start_date: Start date of the month (defaults to current month)

        Returns:
            Path to the created Excel file
        """
        if not month_start_date:
            month_start_date = datetime.now().replace(day=1)

        month_str = month_start_date.strftime('%Y_%m')
        filename = f"OceanBase_Monthly_Report_{month_str}"

        # Aggregate monthly data
        monthly_capacity = self._aggregate_reports(daily_reports, 'monthly')

        return self.export_consolidated_report(
            capacity_data=monthly_capacity,
            tenants_data=[],  # You can aggregate tenant data similarly
            report_frequency='Monthly',
            custom_filename=filename
        )

    def _aggregate_reports(self, reports: List[Dict], frequency: str) -> List[Dict]:
        """
        Aggregate multiple reports based on frequency

        Args:
            reports: List of report dictionaries
            frequency: 'weekly' or 'monthly'

        Returns:
            Aggregated report data
        """
        if not reports:
            return []

        # For now, return the most recent report
        # You can implement more sophisticated aggregation logic
        # (e.g., averaging metrics, summing totals, etc.)
        return reports
